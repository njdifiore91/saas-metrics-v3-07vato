# Logstash Pipeline Configuration v7.17.0
# Purpose: Log aggregation and processing for Startup Metrics Platform

input {
  # Filebeat, Metricbeat, and Heartbeat input
  beats {
    port => "${BEATS_PORT}"
    ssl => true
    ssl_certificate_authorities => ["/etc/logstash/certs/ca.crt"]
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_verify_mode => "peer"
    client_inactivity_timeout => 60
  }

  # TCP input for direct log shipping
  tcp {
    port => 5000
    codec => json_lines
    ssl_enable => true
    ssl_cert => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
  }

  # HTTP input for REST API logs
  http {
    port => 8080
    codec => json
    ssl => true
    keystore => "/etc/logstash/keystore.jks"
    keystore_password => "${KEYSTORE_PASSWORD}"
    threads => 4
  }
}

filter {
  # Grok pattern matching for structured logging
  grok {
    patterns_dir => ["/usr/share/logstash/patterns", "/etc/logstash/custom_patterns"]
    match => {
      "message" => [
        "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} [%{DATA:service}] [%{DATA:trace_id}] %{GREEDYDATA:message}",
        "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{DATA:component} - %{GREEDYDATA:message}"
      ]
    }
    pattern_definitions => {
      "METRIC_PATTERN" => "%{NUMBER:value}|%{WORD:metric_name}|%{DATA:dimensions}"
    }
    keep_empty_captures => false
    tag_on_failure => ["_grokparsefailure"]
  }

  # Timestamp processing
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
    timezone => "UTC"
    tag_on_failure => ["_dateparsefailure"]
  }

  # JSON parsing for structured data
  json {
    source => "message"
    target => "parsed_json"
    skip_on_invalid_json => true
    add_tag => ["json_processed"]
    tag_on_failure => ["json_parse_failure"]
  }

  # Field mutations and enrichment
  mutate {
    add_field => {
      "environment" => "${ENV:production}"
      "service_name" => "${SERVICE_NAME}"
      "datacenter" => "${DC_NAME}"
      "host_ip" => "%{[host][ip]}"
    }
    convert => {
      "response_time" => "float"
      "status_code" => "integer"
    }
    remove_field => ["@version", "host"]
  }

  # Add error handling tags
  if "_grokparsefailure" in [tags] {
    mutate {
      add_field => { "parse_error" => "Failed to parse log message structure" }
    }
  }

  if "_dateparsefailure" in [tags] {
    mutate {
      add_field => { "parse_error" => "Failed to parse timestamp" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS}"]
    index => "startup-metrics-%{+YYYY.MM.dd}"
    template_name => "startup-metrics"
    template_overwrite => true
    manage_template => true
    document_type => "_doc"
    retry_on_conflict => 3
    bulk_max_size => 1000
    flush_size => 500
    timeout => 30

    # SSL Configuration
    ssl => true
    ssl.verification_mode => "full"
    ssl.certificate_authority => "/etc/logstash/certs/ca.crt"
    ssl.truststore.path => "/etc/logstash/truststore.jks"
    ssl.truststore.password => "${TRUSTSTORE_PASSWORD}"

    # Index Lifecycle Management
    ilm_enabled => true
    ilm_rollover_alias => "startup-metrics"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "startup-metrics-policy"
    pipeline => "startup-metrics-pipeline"
  }

  # Error handling for failed documents
  if "_elasticsearch_bulk_api_error" in [tags] {
    file {
      path => "/var/log/logstash/failed_documents.log"
      codec => json
    }
  }
}

# Performance tuning
pipeline.workers: ${PIPELINE_WORKERS}
pipeline.batch.size: 125
pipeline.batch.delay: 50
queue.type: ${QUEUE_TYPE}
queue.max_bytes: ${QUEUE_MAX_BYTES}